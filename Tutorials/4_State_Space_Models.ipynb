{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n\n<h1 align=\"center\">Hidden Markov Models Tutorial</h1> \n<h3 align=\"center\">Summer Workshop on the Dynamic Brain</h3> \n<h3 align=\"center\">Friday, August 26th, 2022</h3> \n<h4 align=\"center\">Scott Linderman</h4> ","metadata":{},"id":"2803a66b-010f-4b4b-97ec-c42ec3bbe1a9"},{"cell_type":"markdown","source":"<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n\n<p>This notebook will introduce you to hidden Markov models (HMMs), a type of probabilistic state space model. HMMs are models for sequential data, like a neural spike train or a behavioral time series. The key idea is that the observed data arises from an underlying sequence of <i>discrete latent states</i>. For example, maybe a neural population switches between an up and down state; or maybe a mouse switches between an attentive and inattentive state, which manifests in its behavior. By the end of this tutorial, you will have tools to infer these states from observed data.\n</div>\n\n<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n    <p>We'll use a software library called <a href=\"https://github.com/probml/ssm-jax\">SSM-JAX</a>, which I am developing along with a team of researchers and summer students at Google. Eventually, I plan to deprecate my lab's <a href=\"https://github.com/lindermanlab/ssm/\">SSM</a> library with this one, which uses <a href=\"https://github.com/google/jax\">JAX</a>, a streamlined and improved version of TensorFlow.\n</div>","metadata":{},"id":"4aa1228c-9b35-492d-980d-38038a059876"},{"cell_type":"code","source":"%pip install -U -q git+https://github.com/probml/ssm-jax.git@swdb","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"cb2ab237-d04f-497e-bd0f-a0984fb14069"},{"cell_type":"code","source":"import os\nimport numpy as np\nimport numpy.random as npr\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import trange\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d8f2a28b-2a25-4abb-ba8f-ac595ff7b29a"},{"cell_type":"markdown","source":"# 1. Simulate data from a Poisson HMM\n\nAs a warm-up, let's simulate a simple neuron that switches between an \"up\" state (1) and a \"down\" state (0). In the up state, the neuron fires at a rate of $\\lambda_1 = 50$ spikes/sec, and in the down state the neuron fires at a rate of $\\lambda_0=10$ spikes/sec. Assume that have binned time into $\\Delta = 25$ms bins. \n\nLet $z_t \\in \\{0, 1\\}$ denote the discrete latent state in time bin $t$, and let $x_t \\in \\mathbb{N}$ denote the number of spikes the neuron fired in that bin. Assume that the spike counts are conditionally distributed according to a Poisson distribution, given the discrete latent state,\n\\begin{align*}\nx_t \\mid z_t &\\sim \\mathrm{Po}(\\lambda_{z_t} \\cdot \\Delta)\n\\end{align*}\n\nFurthermore, assume that the discrete states tend to persist for a number of time bins before switching. We can model that with a transition distribution that places high probability on self-transitions ($z_{t+1} = z_t$).\n\\begin{align*}\np(z_{t+1} \\mid z_t) &= \n\\begin{cases}\n0.9 &\\text{if } z_{t+1} = z_t \\\\\n0.1 &\\text{if } z_{t+1} \\neq z_t \\\\\n\\end{cases}\n\\end{align*}\nWe can represent the transtion probabilities as a matrix $P$ where the entry $P_{ij} = p(z_{t+1} = j \\mid z_t = i)$. In this case,\n\\begin{align*}\nP &= \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.1 & 0.9 \\end{bmatrix}\n\\end{align*}\n\nFinally, assume the initial state is drawn from a uniform distribution, $p(z_1 = 0) = p(z_1 = 1) = 0.5$.","metadata":{},"id":"b462502d-e0ae-4d3c-a67d-3e2f1642ee8e"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 1:</b> Simulate 10 seconds of data of a neuron according to this 2-state HMM. <i>Hint:</i> Use <code>npr.choice</code> to sample the discrete states and <code>npr.poisson</code> to sample the spike counts.\n</div>","metadata":{},"id":"99245f24-4fb1-4aaa-bc3d-09d8bbcc15db"},{"cell_type":"code","source":"# Uncomment and run the following line to get help with the `choice` function.\n# npr.choice?","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"97a75e9b-38c6-4f87-a900-6672319b3f69"},{"cell_type":"code","source":"# Uncomment and run the following line to get help with the `poisson` function.\n# npr.poisson?","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d9141b74-9f34-4373-ad92-bf23839ed0a3"},{"cell_type":"code","source":"npr.seed(1)\n\n# Compute the integer number of time bins in the simulation\nbin_size = 0.025  # in sec\ntotal_time = 10.0  # in sec\nnum_timebins = ... # TODO\n\n# Specify the number of neurons\nnum_neurons = 1\n\n# Set the model parameters\ninitial_distribution = np.array([0.5, 0.5])\ntransition_matrix = np.array([[0.9, 0.1], \n                              [0.1, 0.9]])\nfiring_rates = np.array([[10.], \n                         [50.]])\n\n# Initialize the outputs\nstates = np.zeros(num_timebins, dtype=int)\nspike_counts = np.zeros((num_timebins, num_neurons), dtype=int)\n\n# Sample the first state and observation\nstates[0] = npr.choice(p=...)        # TODO\nspike_counts[0] = npr.poisson(...)   # TODO\n\n# Sample subsequent time steps\nfor t in range(1, num_timebins):\n    states[t] = ... # TODO\n    spike_counts[t] = ... # TODO","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"20da3739-d0dc-4a82-b7a0-3dde3f5d3c98"},{"cell_type":"code","source":"# Plot the simulated data\nplt.imshow(states[None, :], cmap=\"Greys\", alpha=0.25, aspect=\"auto\", \n           extent=(0, num_timebins, 0, 6))\nplt.plot(spike_counts, 'o')\nplt.xlabel(\"time bin\")\nplt.ylabel(\"spike count\")\n\ncb = plt.colorbar()\ncb.set_label(\"discrete state\")\ncb.set_ticks([0, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2302a667-1434-4041-93c8-b7b204d7cfa7"},{"cell_type":"markdown","source":"# 2. Fitting an HMM to data with Expectation-Maximization\nLet's see if we can recover the model parameters and most likely states.","metadata":{},"id":"0504cf54-2afd-402d-959f-1b710bf08f25"},{"cell_type":"code","source":"import jax.numpy as jnp\nimport jax.random as jr\n\nfrom ssm_jax.hmm.models import PoissonHMM","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"b19f4424-c2fa-41d2-b812-d4e9081e785c"},{"cell_type":"code","source":"# The fitting code assumes that you give it a batch of time series, so the first\n# thing we'll do is add an extra dimension\nbatch_of_spike_counts = jnp.array([spike_counts])\nprint(batch_of_spike_counts.shape) \n# Output: (batch size, num_timebins, num_neurons)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"7a3cadd3-38d0-461d-b0df-67b17cf6ccca"},{"cell_type":"code","source":"# Construct an HMM with Poisson emissions using randomly initialized parameters\nhmm = PoissonHMM.random_initialization(jr.PRNGKey(0), num_states=2, emission_dim=1)\n\n# Now fit the HMM using expectation-maximization (EM)!\nlps = hmm.fit_em(batch_of_spike_counts, num_iters=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"ec14ef31-47fd-4552-8a63-e6571d2def90"},{"cell_type":"code","source":"plt.plot(lps)\nplt.xlabel(\"EM iteration\")\nplt.ylabel(\"Log Probability\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"88d7aa72-81b6-4661-84ef-23b40d9cdd23"},{"cell_type":"code","source":"# Look at the estimated parameters\nprint(\"estimated rates: \")\nprint(hmm.emission_rates.value / bin_size)\nprint(\"\")\nprint(\"estimated transition matrix: \")\nprint(hmm.transition_matrix.value)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"57e2625c-92fd-42b9-91cb-23701fae1d27"},{"cell_type":"code","source":"# Find the most likely discrete states given the learned model parameters\nmost_likely_states = hmm.most_likely_states(spike_counts)\n\n# Compare the true and inferred states\nfig, axs = plt.subplots(2, 1, sharex=True)\naxs[0].plot(states)\naxs[0].set_ylabel(\"true state\")\naxs[1].plot(most_likely_states)\naxs[1].set_ylabel(\"inferred state\")\naxs[1].set_xlabel(\"time bin\")\nplt.xlim(0, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e3ef4ed5-eb35-4695-b58c-319ffc5d690f"},{"cell_type":"markdown","source":"The HMM yields more than just the most likely states, it gives a full posterior distribution over states. The posterior probabilities are the results of running the HMM **smoother** to compute $p(z_t \\mid x_{1:T})$ for each time bin $t$. ","metadata":{},"id":"ecde2261-6086-42a2-b970-33d775247d1d"},{"cell_type":"code","source":"# Find the posterior distribution over states\nposterior = hmm.smoother(spike_counts)\n\n# Compare the true and inferred states\nplt.imshow(states[None, :], cmap=\"Greys\", alpha=0.25, aspect=\"auto\", \n           extent=(0, num_timebins, 0, 1))\nplt.plot(posterior.smoothed_probs[:, 0])\nplt.xlabel(\"time bin\")\nplt.ylabel(\"discrete state probability\")\nplt.xlim(0, 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"8c6f8a37-59da-45f6-8d2a-4da91b938d55"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 2.1:</b> Did the model recover the true states? How close should we expect it to get?\n</div>","metadata":{},"id":"544a11f2-323c-4e35-92d2-cef9eb739b9b"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 2.2:</b> What changes to the parameters would make the states easier or harder to infer? Why?\n</div>","metadata":{},"id":"d570b414-431a-41a9-949a-16344ca05b49"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 2.3:</b> Change the simulation code to simulate multiple neurons that all share the same underlying discrete state. \n</div>\n\nThat is, let $\\mathbf{x}_t = (x_{t1}, \\ldots, x_{tN})^\\top$ denote a vector of spike counts for each of $N$ neurons in time bin $t$. Assume the neurons' spike counts are conditionally independent Poisson random variables given the discrete state,\n\\begin{align*}\n    p(\\mathbf{x}_t \\mid z_t) &= \\prod_{n=1}^N \\mathrm{Po}(x_{tn} \\mid \\lambda_{z_t,n} \\Delta)\n\\end{align*}\nwhere $\\lambda_{z_t,n} \\in \\mathbb{R}_+$ is the firing of neuron $n$ in state $z_t$.","metadata":{},"id":"1b9e6c19-6c38-46c2-8a82-7db223dfb46e"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 2.4:</b> Would adding more neurons make the discrete states harder or easier to infer? Why?\n</div>","metadata":{},"id":"da70cce9-1f6e-46a1-9aad-563f2e7a8f42"},{"cell_type":"markdown","source":"# 3. Load and preprocess the Visual Behavior Neuropixels data\n\nNext, we'll develop a simple model of the licking behavior in the visual behavior dataset. Alex Piet at the Allen Institute has developed similar models to look for changes in the animals' internal state.","metadata":{},"id":"ce53d97a-7856-45ff-a588-477b097ccd38"},{"cell_type":"code","source":"import platform\nplatstring = platform.platform()\n\ndata_dirname = 'visual-behavior-neuropixels'\nuse_static = False\nif 'Darwin' in platstring or 'macOS' in platstring:\n    # macOS \n    data_root = \"/Volumes/Brain2022/\"\nelif 'Windows'  in platstring:\n    # Windows (replace with the drive letter of USB drive)\n    data_root = \"E:/\"\nelif ('amzn' in platstring):\n    # then on AWS\n    data_root = \"/data/\"\n    data_dirname = 'visual-behavior-neuropixels-data'\n    use_static = True\nelse:\n    # then your own linux platform\n    # EDIT location where you mounted hard drive\n    data_root = \"/media/$USERNAME/Brain2022/\"","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"892227c5-0862-41c8-9ca1-e7e0ebfe96cf"},{"cell_type":"code","source":"from allensdk.brain_observatory.behavior.behavior_project_cache.\\\n    behavior_neuropixels_project_cache \\\n    import VisualBehaviorNeuropixelsProjectCache\n\n# this path should point to the location of the dataset on your platform\ncache_dir = os.path.join(data_root, data_dirname)\n\ncache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(\n            cache_dir=cache_dir, use_static_cache=use_static)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5a41a8b7-aa4b-4195-a2f8-d4a7e6f42128"},{"cell_type":"code","source":"# Load our favorite session (the one from the VBN tutorial!)\nsession_id = 1053941483\nsession = cache.get_ecephys_session(\n            ecephys_session_id=session_id)\nsession.metadata","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"175dfe6f-ce11-4a40-bcc4-b47b1fe918c8"},{"cell_type":"markdown","source":"### Get the active stimulus presentations (aka flashes) and lick data\nWe'll only study the active flashes in this tutorial, since we'll be modeling the licking data.","metadata":{},"id":"77385759-914a-4bbf-b181-adce8bf38361"},{"cell_type":"code","source":"licks = session.licks\nflashes = session.stimulus_presentations[session.stimulus_presentations.active].copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"28bc85cc-0204-4c8a-be6c-e9a383eb21a2"},{"cell_type":"markdown","source":"### Find the onset of lick bouts\n\nLicks come in short bouts. Find the first lick in each bout. Then, for each flash, add a boolean flag to specify if the mouse started a lick bout following that presentation. ","metadata":{},"id":"3ecf46c1-8d1e-4359-a2c1-12ae15b530da"},{"cell_type":"code","source":"lick_times = licks.timestamps.values\nlick_intervals = np.concatenate([[np.inf], np.diff(lick_times)])\nis_bout_start = lick_intervals > 3 * np.median(lick_intervals[1:])\nbout_start_times = lick_times[is_bout_start]\nflashes['licked'] = flashes.apply(\n    lambda row: np.sum((bout_start_times > row[\"start_time\"]) & \\\n                       (bout_start_times < row[\"start_time\"] + 0.75)) > 0,\n    axis=1,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"09551030-7e3e-4795-bf20-30c699441c5e"},{"cell_type":"markdown","source":"## Visualize the mouse's performance over the session","metadata":{},"id":"2869c7cf-1820-4723-a366-828b4c7b260d"},{"cell_type":"code","source":"change_idxs = np.where(flashes.is_change)[0]\nomitted_idxs = np.where(flashes.omitted)[0]\nlick_idxs = np.where(flashes.licked)[0]\nplt.plot(lick_idxs, np.ones_like(lick_idxs), 'ro', ms=10, mec='w', label=\"lick\")\nplt.plot(change_idxs, np.ones_like(change_idxs), 'o', mec='k', mfc='none', ms=12, mew=3, label=\"change\")\nplt.plot(omitted_idxs, np.ones_like(omitted_idxs), 'o', mec='b', mfc='none', ms=12, mew=3, label=\"omitted\")\nplt.xlim(1250, 1350)\nplt.yticks([])\nplt.legend()\nplt.xlabel(\"flash index\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"d403faa7-cb18-46c2-8803-8a3dc486ebda"},{"cell_type":"markdown","source":"### Extract some summary statistics of the mouse's performance over time","metadata":{},"id":"83fe6482-9c9f-4c07-8b5b-ac90646762c7"},{"cell_type":"code","source":"# Label licks as:\n# - true positive (hits)\n# - false negative (miss)\n# - false positive (mostly abort)\n# - true negative (correct reject)\ntp = np.array(flashes.is_change & flashes.licked, dtype=float)\nfn = np.array(flashes.is_change & ~flashes.licked, dtype=float)\nfp = np.array(~flashes.is_change & flashes.licked, dtype=float)\ntn = np.array(~flashes.is_change & ~flashes.licked, dtype=float)\n\n# Smooth these over time with a Gaussian filter to estimate \"rates\"\nfrom scipy.ndimage import gaussian_filter1d\nsigma = 10\ntpr = gaussian_filter1d(tp, sigma)\nfnr = gaussian_filter1d(fn, sigma)\nfpr = gaussian_filter1d(fp, sigma)\ntnr = gaussian_filter1d(tn, sigma)\n\n# Plot them\nplt.plot(tpr, label=\"hit rate\")\nplt.plot(fnr, label=\"miss rate\")\nplt.plot(fpr, label=\"abort rate\")\nplt.plot(tnr, label=\"correct reject rate\")\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"215701bc-4e4d-4e6d-a009-0c9431e53504"},{"cell_type":"markdown","source":"Rather than looking at all four of these quantities, we often summarize them in terms of the _precision_ (here, the fraction of licks that were hits) and _recall_ (here, the fraction of changes that the mouse hit). ","metadata":{},"id":"0f210c9f-94c6-4482-a301-946d58e5d378"},{"cell_type":"code","source":"# Compute precision and recall rates\nprecision = tpr / (tpr + fpr + 1e-3)\nrecall = tpr / (tpr + fnr + 1e-3)\n\n# Plot a smoothed estimate of the true positive (etc) rate over time\nfig, axs = plt.subplots(2, 1, sharex=True)\naxs[0].plot(precision)\naxs[0].set_ylabel(\"precision\")\naxs[0].set_ylim(0, 1)\naxs[1].plot(recall)\naxs[1].set_ylabel(\"recall\")\naxs[1].set_ylim(0, 1)\naxs[1].set_xlabel(\"stimulus presentation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"3df5dca6-057c-442c-af38-cd1441d80c54"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 3.1:</b> Is the mouse's performance consistent over the whole session? If not, how would you describe the changes in performance?\n</div>","metadata":{},"id":"db240bf9-6d46-4302-8944-6c5a7e1e2840"},{"cell_type":"markdown","source":"# 4. Model the behavior with a switching logistic regression\n\nThe idea behind this model is that the mouse's behavior varies with changes in its internal state. We don't directly observe this internal state (it's latent), but we can draw inferences about it based on the mouse's behavior. For example, if the mouse stops licking in response to stimulus changes, then we might infer that it has switched into an \"inattentive\" or \"disenganged\" state. The internal states we infer from behavior can offer clues for when we should look for changes in the underlying neural activity.\n\nLet's formalize this with a hidden Markov model. Let \n- $y_t \\in \\{0,1\\}$ specify whether or not the **mouse licked** on stimulus presentation $t$.\n- $x_t \\in \\{-1,1\\}$ specify whether or not the **stimulus changed** on stimulus presentation $t$.\n- $z_t \\in \\{1, \\ldots, K\\}$ denote the **discrete internal state** of the mouse on stimulus presentation $t$.\n\nWe will model the relationship between licking ($y_t$) and stimulus changes ($x_t$) as a function of the internal state ($z_t$) with the following conditional distribution,\n\\begin{align*}\n\\Pr(y_t = 1 \\mid x_t, z_t) &= \\sigma(\\alpha_{z_t} x_t + \\beta_{z_t})\n\\end{align*}\nwhere $\\sigma(u) = \\frac{1}{1+e^{-u}}$ is the _logistic function_, and $(\\alpha_k, \\beta_k)$ are parameters of a _logistic regression_. \n\nThe logistic function is a \"squashing\" nonlinearity,","metadata":{},"id":"b7999d3f-01ce-42a7-972e-921ded8b21e3"},{"cell_type":"code","source":"u = np.linspace(-5, 5, 100)\nplt.plot(u, 1 / (1 + np.exp(-u)))\nplt.xlabel(\"$u$\")\nplt.ylabel(\"$\\sigma(u)$\")\nplt.title(\"logistic function\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"0cc336c4-8ee7-40cd-8f1a-c679441c03d9"},{"cell_type":"markdown","source":"Intuitively, when the linear function $\\alpha_{z_t} x_t + \\beta_{z_t}$ is much greater than zero, the probability of the mouse licking is high. When the linear function is much less than zero, the lick probability is low. \n\nThus, $\\alpha_k$ determines the mouse's _sensitivity_: how much a change in stimulus alters the mouse's lick probability in state $z_t = k$. Likewise, $\\beta_k$ determines the mouse's _bias_ of the mouse toward licking or not licking in state $z_t = k$. \n\nWe are interested in whether the sensitivity and bias of the mouse are changing over the course of the session. If they are, it might signal a change in internal state.\n\nTo do so, we'll fit a `LogisticRegressionHMM`, i.e. an HMM where each state corresponds to a different logistic regression mapping stimulus features to behavior.","metadata":{},"id":"1d76925d-b476-4763-a938-8470e8d69313"},{"cell_type":"code","source":"from ssm_jax.hmm.models import LogisticRegressionHMM","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"08b85d2f-2a64-4c78-b680-4e156b0d3881"},{"cell_type":"markdown","source":"### Construct the features that go into the logistic regression","metadata":{},"id":"f4b2c3bc-dadb-42e5-b899-1f47138c5a2e"},{"cell_type":"code","source":"# Encode stimulus change as +1 and no change as- 1\nlicked = flashes.licked.values.astype(int)\npm1 = lambda x: -1 + 2 * x\nis_change = pm1(flashes.is_change.values.astype(float))\n\n# For this model, it's important that the data is converted into a *JAX* array\ny = jnp.array(licked)\nX = jnp.column_stack([is_change])\nX_names = [\"is change\"]\nn_features = X.shape[1]\n\n# Split the data into equal length batches\nn_batches = 8\nn_flashes = len(y) - (len(y) % n_batches)\nbatched_y = y[:n_flashes].reshape(n_batches, -1)\nbatched_X = X[:n_flashes].reshape(n_batches, -1, n_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"3dafb5e3-217b-41c6-9b8b-bb7836a8d81f"},{"cell_type":"code","source":"num_states = 3\nhmm = LogisticRegressionHMM.random_initialization(\n    jr.PRNGKey(1234), num_states, feature_dim=n_features,\n    emission_matrices_variance=1.0)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"83a4a2f1-9071-442c-9a2c-dd36b9f94818"},{"cell_type":"markdown","source":"### Freeze the initial probabilities and transition matrix\n\nWe're looking for slowly changing internal states. We can enforce that by freezing the transition matrix to assign highest probability to self transitions. In our code, we can freeze parameters by calling `freeze()` on them.","metadata":{},"id":"26dd4b65-71f8-405f-9674-44865a008f41"},{"cell_type":"code","source":"# Fix the initial probabilities and transition matrix\nhmm.initial_probs.value = jnp.ones(num_states) / num_states\nhmm.initial_probs.freeze()\n\nhmm.transition_matrix.value = 0.95 * jnp.eye(num_states) + 0.05 * jnp.ones((num_states, num_states)) / num_states\nhmm.transition_matrix.freeze()","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"5f3682bc-0fa7-4b25-bc26-f3b04edae008"},{"cell_type":"markdown","source":"### Fit the model with EM","metadata":{},"id":"23a4c854-a5f6-4d74-9898-d6a89edf9aef"},{"cell_type":"code","source":"# Now call fit, again casting to JAX arrays\nlps = hmm.fit_em(jnp.array(batched_y), features=jnp.array(batched_X), num_iters=200)","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"e794e29f-73c0-4447-b84f-a4cb518a9cbc"},{"cell_type":"code","source":"plt.plot(lps[1:])\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Log Probability\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"60102b8c-a659-42eb-a860-f42311700a57"},{"cell_type":"markdown","source":"### Let's look at the inferred states!","metadata":{},"id":"ee79d63c-d08a-4c80-aee4-ebc3bc19e717"},{"cell_type":"code","source":"# Find the most likely discrete states given the learned model parameters\nmost_likely_states = hmm.most_likely_states(y, features=X)\n\n# Overlay the precision and recall curves on top of the inferred states\nplt.figure(figsize=(8, 4))\nplt.imshow(most_likely_states[None, :], alpha=0.5, \n           extent=(0, len(y), 0, 1), aspect=\"auto\")\nplt.plot(precision, '-k', label=\"precision\")\nplt.plot(recall, ':k', label=\"recall\")\nplt.ylim(0, 1)\nplt.ylabel(\"precision/recall\")\nplt.xlabel(\"stimulus presentation\")\nplt.legend(loc=\"lower left\")\n\ncb = plt.colorbar()\ncb.set_label(\"discrete state\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"98cdcdba-fe09-4475-94dc-f0a2cec39afb"},{"cell_type":"markdown","source":"As above, it's also a good idea to look at the posterior state probabilities.","metadata":{},"id":"274863b2-e612-44e5-8965-6a76caeb3501"},{"cell_type":"code","source":"# Find the posterior distribution over states\nposterior = hmm.smoother(y, features=X)\n\n# Compare the true and inferred states\nfig, axs = plt.subplots(hmm.num_states, 1, sharex=True)\nfor k, ax in enumerate(axs):\n    ax.plot(posterior.smoothed_probs[:, k])\n    ax.set_ylim(-0.05, 1.05)\n    ax.set_ylabel(\"state {} prob\".format(k))\nplt.xlabel(\"stimulus presentation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"390bbf74-8eaf-456d-b948-cc4fb127c965"},{"cell_type":"markdown","source":"### Let's dig into the model parameters\n\nSpecifically, we'll plot the weights associated with each feature under each discrete state.","metadata":{},"id":"0b93bc1c-66c6-4e03-b7ea-0482f9185c29"},{"cell_type":"code","source":"fig, axs = plt.subplots(1, hmm.num_states, sharey=True, figsize=(8, 4))\nfor k, ax in enumerate(axs):\n    ax.bar(np.arange(X.shape[1]), hmm.emission_matrices.value[k])\n    ax.bar(X.shape[1], hmm.emission_biases.value[k])\n    ax.plot([-0.5, X.shape[1] + .5], [0, 0], '-k')\n    ax.set_xlim(-0.5, X.shape[1] + .5)\n    ax.set_xticks(np.arange(X.shape[1]+1))\n    ax.set_xticklabels(X_names + [\"bias\"], rotation=45)\n    if k == 0: ax.set_ylabel(\"weight\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"2f9937ba-e93b-4c8f-b627-62001496aefb"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 4.1:</b> How does the mouse's behavior change across these three states? How does that manifest in the weights of the logistic regression?\n</div>","metadata":{},"id":"393925f4-00d6-42b1-9358-2e0d9151d7c9"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 4.2:</b> What other features could you add to the logistic regression? Modify the code above to add them, then refit the model and see how the inferred states and parameters change.\n</div>","metadata":{},"id":"423b25be-23dd-4c73-b9f8-cd50d1e001fa"},{"cell_type":"markdown","source":"# 5. Cross validation\n\nPerform cross validation to select the number of discrete states.\n\n**This will take a few minutes so grab a coffee and hang tight!**","metadata":{},"id":"9cd9f585-4507-4dc3-8b29-c124e02be4c6"},{"cell_type":"code","source":"avg_test_log_probs = []\nfor num_states in range(1, 11):\n    print(\"num states: \", num_states)\n    \n    test_log_probs = []\n    for batch in trange(n_batches):\n        # Extract all but this batch for training\n        train_X = np.concatenate([batched_X[:batch], batched_X[batch+1:]])\n        train_y = np.concatenate([batched_y[:batch], batched_y[batch+1:]])\n\n        # Make an HMM\n        hmm = LogisticRegressionHMM.random_initialization(\n            jr.PRNGKey(0), num_states, feature_dim=n_features,\n            emission_matrices_variance=1.0)\n        \n        # Fix the initial probabilities and transition matrix\n        hmm.initial_probs.value = jnp.ones(num_states) / num_states\n        hmm.initial_probs.freeze()\n        hmm.transition_matrix.value = 0.95 * jnp.eye(num_states) + 0.05 * jnp.ones((num_states, num_states)) / num_states\n        hmm.transition_matrix.freeze()\n        \n        # Fit the model\n        lps = hmm.fit_em(jnp.array(train_y), features=jnp.array(train_X), num_iters=200, verbose=False)\n        \n        # Evaluate the log probability on held out data\n        test_lp = hmm.marginal_log_prob(jnp.array(batched_y[batch]), \n                                        features=jnp.array(batched_X[batch]))\n        test_log_probs.append(test_lp)\n       \n    # Store the average test log prob\n    avg_test_log_probs.append(np.mean(test_log_probs))\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"22fd8708-7460-47ee-9362-5818147c3f21"},{"cell_type":"code","source":"plt.plot(np.arange(1, len(avg_test_log_probs) + 1), avg_test_log_probs, '-ko')\nplt.xlabel(\"number of states\")\nplt.ylabel(\"average test log prob\")","metadata":{"trusted":true},"execution_count":null,"outputs":[],"id":"df150410-a7d8-4425-97b0-54354c91bb23"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 5.1:</b> Do the cross-validation results match your expectations?  What other metrics could help guide your choice of number of states?\n</div>","metadata":{},"id":"8329426a-0663-4f71-b2c9-a5606fb9e339"},{"cell_type":"markdown","source":"# 6. Thought Problems (and Project Ideas)","metadata":{},"id":"010afd0b-c3ad-49f4-98f6-edaf1fa5f478"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 6.1:</b> You can think of the inferred behavioral states as features that have been derived from the behavioral data. These features are prime candidates to correlate with neural measurements. Do you think the neural responses to flashes/changes/omissions will differ from behavioral state to the next? \n</div>","metadata":{},"id":"af94b0ea-20e3-4f20-a6f6-8e36208649b7"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 6.2:</b> Likewise, we can correlate the inferred behavioral states with other aspects of the animal's behavior. Have we simply constructed a fancy way of extracting running speed and/or pupil size?\n</div>","metadata":{},"id":"f1582fc7-5586-463d-aca6-15867c51157c"},{"cell_type":"markdown","source":"<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n    <p><b>Problem 6.3:</b> How do the behavioral states differ across mice or sessions? Are some mice better performers than others?\n</div>","metadata":{},"id":"46119e0c-7a60-4938-b7ca-eb140ea7e4d1"}]}