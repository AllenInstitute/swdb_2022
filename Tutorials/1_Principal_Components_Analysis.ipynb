{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Dimensionality Reduction SWDB 2022 </h1> \n",
    "<h3 align=\"center\">Monday, August 29, 2022</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we will use.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In this tutorial, we look at dimensionality reduction. Dimensionality reduction is the process of taking high dimesnional data (such as a population of recorded neurons) and trasforming it it into a space that is lower dimesional and (hopefully) easier to understand. We will go over one of the most common and important forms of dimensionality reduction; <b>Principal Components Analysis</b> (PCA).\n",
    "</p>\n",
    "    \n",
    "<p> PCA works by identifying the dimensions of a dataset that contain the most variance (more on this to come!). Because this dimensionality reduction is based on the inherent structure of the data, it is one of the most common forms of <b>unsupervised learning</b>.  In contrast to supervised learning, unsupervised learning operates on a set of data points without labels or ids.  Instead of trying to find a transformation between the data and labels attached to the data, we seek to find a transformation that discovers structure in the data.\n",
    "</p>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #FBE9E7; \">\n",
    "<p>\n",
    "A mathematical definition of PCA follows; The explaination above can be a little mistifying if you haven't had enough coffee and/or if it has been a minute since your last linear algebra class. We will spend more time on the pictographic version below, but there is here for a reference for those who need it!\n",
    "</p>     \n",
    "\n",
    "<p>\n",
    "Lets assume the data $\\vec{x}_i$ exists in $N$ dimensions.  Given an integer $n < N$, PCA attemps to find a linear subspace of dimension $n$ that minimizes the variance of the data outside of that subspace.  Put another way, PCA defines a subspace of dimension $n < N$ such that, when the data is approximated by projecting onto that subspace, the reconstruction error is minimized. In short, PCA finds the dimensions along which the data is most spread out. These dimensions are not necessarily those present within the data, but more often are linear combinations of them.\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "Let us denote the subspace with the orthogonal matrix ${\\bf W}$, of dimensions $(n, N)$.  The data can be represented with the matrix ${\\bf X}$, of dimensions $(T, N)$, where $T$ is the number of samples.  Let the coordinates of the data in the subspace be labelled ${\\bf Z}$, which is of dimensions $(T, n)$.  The cost function for PCA is then\n",
    "</p>\n",
    "<p>\n",
    "$E = \\frac{1}{2}  \\left | {\\bf X} - {\\bf Z} \\cdot {\\bf W} \\right |^2$\n",
    "</p>\n",
    "<p>\n",
    "    Note that we have to optimize over <i>both</i> ${\\bf Z}$ and ${\\bf W}$, subject to the constraint that ${\\bf W}$ is orthogonal.\n",
    "</p>\n",
    "<p>\n",
    "We can equivalently define PCA by specifying the principal components as the eigenvectors of the covariance matrix with the $n$ largest eigenvalues.  Intuitively one can see that this choice will produce the smallest amount of variability away from the subspace, as the eigenvectors themselves indicate the direction of maximum variability, and thus will solve the problem as we originally defined it.  This allows us to compute the PCAs very simply with diagonalization or singular value decomposition.\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "To understand PCA intuitivly, consider this whale shark (Borrowed shamelessly from #AllisonHorst on twitter):\n",
    "</p>\n",
    "\n",
    "<img src=\"../resources/PCAWhale1.png\" width = 500>  \n",
    "    \n",
    "<p>\n",
    "Now, imagine that the shark needs to eat this school of krill:\n",
    "</p>\n",
    "\n",
    "<img src=\"../resources/PCAWhale2.png\" width = 500>  \n",
    "    \n",
    "<p>\n",
    "Principal components analysis provides a an algorithm that the shark can use to maximize how many krill will end up in its mouth. \n",
    "</p>\n",
    "    \n",
    "   \n",
    "<p> \n",
    "To see how this works, lets start by imaging that the shark has a couple of footballs (this is, obviously, a super realistic example). To find the widest dimension of krill the shark will need to engulf, they can try to hold a football such that it entirly covers the krill population. The widest dimension will be the one in which the smallest football can cover all of the krill. Another way of saying this is that the football needs to be positioned to minimze the travel of any given krill trying to leave it.\n",
    "</p>\n",
    "    \n",
    "<img src=\"../resources/KrillPlot1.png\"  width = 500>  \n",
    "\n",
    "<img src=\"../resources/KrillPlot2.png\"  width = 500>  \n",
    "\n",
    "    \n",
    "<p>\n",
    "Mathematically, the vector of the long axis of this football is known as the first eigenvector of the covariance matrix of the data, that is, the vector that describes the most variance in our data/krill. In principal components analysis, this is called the first principal component. The amount the shark needs to resize the football is proportional to a quanitity know as the first eigenvalue. \n",
    "    \n",
    "</p>\n",
    "    \n",
    "<p>  \n",
    "Now that the whale-shark has established this first vector, the next step is to establish a second. This is done by compressing all of the krill along the vector established by the first football, then fitting a second football to the remaining data.\n",
    "</p>\n",
    "    \n",
    "<img src=\"../resources/KrillPlot4.png\"  width = 500>  \n",
    "        \n",
    "<p>\n",
    "We call the vector of this second football the second eigenvector of the covariance matrix, that is, the second principal component. The amount that this football is resized scales with the second eigenvalue. \n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "Importantly, this algorithm enforces that that the direction of each football be orthogonal to one another. If we had additional dimensions, it would be enforced for the relationships between all of the principle components. If you hate this assumtion, there is a class of algorithms you can look into called 'independant components analysis,' which is similar to PCA but does require that your footballs be orthogonal to each other.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "In the case of our whale shark, the principle components are directly interpretable. The first principle component tells the shark how to rotate its mouth to fit around the school of krill, and the projections of the krill onto the second principle component tell the shark how far it needs to open its mouth. Another way of saying this is that we have found a linear transform from shark-space to krill space. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by actually implementing this algorithm on some toy data. You may notice a striking similarity to the krill above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random 2 dimensional data\n",
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 300))).T\n",
    "\n",
    "#plot random data\n",
    "plt.plot(X[:, 0], X[:, 1], '.');\n",
    "plt.ylim(-3,3);\n",
    "plt.xlim(-3,3);\n",
    "\n",
    "# Note that this dataset has a mean of 0 in both dimensions. This will be important later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X now has 300 data points, each with 2 dimensions\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Covariance Matrix. \n",
    "# Because X has 2 dimensions, it will be a square 2x2 matrix\n",
    "cov = (1.0/X.shape[0])*np.dot(X.T, X)\n",
    "# Alternativly: A = np.cov(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, do the eigenvalue decomposition of this matrix\n",
    "evalues, evectors = np.linalg.eig(cov)\n",
    "print('evalues: ' + str(evalues))\n",
    "print('evectors: ' + str(evectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Lets look at how these eigenvectors fall relative to our data\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.plot(X[:, 0], X[:, 1], '.')\n",
    "for value, vector in zip(evalues, evectors.T):\n",
    "    ax.plot([0, 3.*np.sqrt(value)*vector[0]], [0, 3.*np.sqrt(value)*vector[1]], '-k', lw=3);\n",
    "\n",
    "ax.set_ylim(-3,3);\n",
    "ax.set_xlim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In this case, our PCA is an orthogonal coordinate system transformation that prioritizes maximum variance. However, one can already see that the majority of the variance in our data is explained by a single principal component!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the values onto the each of the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto first eigenvector\n",
    "X_proj_1 = np.dot(X, evectors.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have one projection value for each data point\n",
    "X_proj_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize!\n",
    "plt.plot(X_proj_1, np.zeros(X_proj_1.shape), '.');\n",
    "plt.xlim(-3,3);\n",
    "plt.xlabel(\"First principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto the second eigenvector\n",
    "X_proj_2 = np.dot(X, evectors.T[1])\n",
    "\n",
    "# Visualize!\n",
    "plt.plot(X_proj_2, np.zeros(X_proj_2.shape), '.');\n",
    "plt.xlim(-3,3);\n",
    "plt.xlabel(\"Second principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, all our principal components transform has done is rotate the data into a new coordinate space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_proj_1, X_proj_2, '.');\n",
    "plt.xlabel(\"First principal component\", fontsize=14)\n",
    "plt.ylabel(\"Second principal component\", fontsize=14)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "### Now let's use sklearn.\n",
    "    \n",
    "\n",
    "\n",
    " <p>\n",
    "\n",
    "<code>scikit-learn</code> provides a more streamlined interface for performing a Principal Components Analysis. Here we will use it first on the same toy data as above, and then on some data from the Behavior recordings. The interface will be the same for other algorithms in <code>scikit-learn</code>, such as those you saw in the regression tutorial\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, declare a PCA object that can be used for computing PCAs\n",
    "# If you don't specify the number of components, the 'default' is as many dimensions as your data has\n",
    "# This won't change the output of the algorithm, just how many PCs sklearn returns\n",
    "pca = PCA(n_components=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, do a dimensionality reduction on X\n",
    "pca.fit(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PCA object automatically does the same computation we did above, though it uses some different nomenclature\n",
    "# the Eigenvalues are stored as 'explained variance_'\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the eigenvectors are stored as \"components\"\n",
    "pca.components_\n",
    "# Compare this to the values above... are they the same? (Hint: They should be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot these data to verify that everything matches up.\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.plot(X[:, 0], X[:, 1], 'o')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    ax.plot([0, v[0]], [0, v[1]], '-k', lw=3);\n",
    "\n",
    "ax.set_ylim(-3,3);\n",
    "ax.set_xlim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the data projected onto the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_project = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This, too, should look a lot like the plot above.\n",
    "plt.plot(X_project[:,0], X_project[:,1], 'o');\n",
    "plt.xlabel(\"First principal component\", fontsize=14);\n",
    "plt.ylabel(\"Second principal component\", fontsize=14);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first principal component explains most of the variance, we can also project our two-dimensional data into a one-dimensional space (making use of the dimensionality-reduction that can be accomplished with PCA!) Note the similarity between this projection and the full data. This will be similar to what happens to high-dimensional data that is reduced to two dimensions for the examples below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting only the first principal component\n",
    "plt.plot(X_project[:,0], np.zeros(len(X_project[:,0])), 'o');\n",
    "plt.xlabel(\"First principal component\", fontsize=14);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "## Let's try this with some real data from the Visual Behavior dataset!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allensdk\n",
    "from allensdk.brain_observatory.\\\n",
    "    behavior.behavior_project_cache.\\\n",
    "    behavior_neuropixels_project_cache \\\n",
    "    import VisualBehaviorNeuropixelsProjectCache\n",
    "import os\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "data_dirname = 'visual-behavior-neuropixels'\n",
    "use_static = False\n",
    "if 'Darwin' in platstring or 'macOS' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2022/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on AWS\n",
    "    data_root = \"/data/\"\n",
    "    data_dirname = 'visual-behavior-neuropixels-data'\n",
    "    use_static = True\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2022/\"\n",
    "\n",
    "data_root =  '../Data'\n",
    "# get the cache location\n",
    "cache_dir = os.path.join(data_root, data_dirname)\n",
    "\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=cache_dir)\n",
    "#cache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(\n",
    "#            cache_dir=cache_dir, use_static_cache=use_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to find which units have the sessions we are interested in, so we will need the units and sessions tables.\n",
    "units_table = cache.get_unit_table()\n",
    "ecephys_sessions_table = cache.get_ecephys_session_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want to find a \"Good\" Session to look at;\n",
    "# For now, we are going to grab the one with the most MRN units.\n",
    "unit_by_session = units_table.join(ecephys_sessions_table,on = 'ecephys_session_id')\n",
    "unit_in = unit_by_session[(unit_by_session['structure_acronym']=='MRN') & (unit_by_session['experience_level']=='Familiar')]\n",
    "unit_count = unit_in.groupby([\"ecephys_session_id\"]).count()\n",
    "familiar_session_with_most_in_units = unit_count.index[np.argmax(unit_count['ecephys_probe_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually imort the data\n",
    "session = cache.get_ecephys_session(ecephys_session_id=familiar_session_with_most_in_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unit information\n",
    "session_units = session.get_units()\n",
    "# Channel information\n",
    "session_channels = session.get_channels()\n",
    "# And accosiate each unit with the channel on which it was found with the largest amplitude\n",
    "units_by_channels= session_units.join(session_channels,on = 'peak_channel_id')\n",
    "\n",
    "# Filter for units in primary visual cortex\n",
    "this_units = units_by_channels[(units_by_channels.structure_acronym == 'MRN')\\\n",
    "                               &(units_by_channels['isi_violations']<.5)\\\n",
    "                               &(units_by_channels['amplitude_cutoff']<0.1)\\\n",
    "                               &(units_by_channels['presence_ratio']>0.95)]\n",
    "# Get the spiketimes from these units as a dictionary\n",
    "this_spiketimes = dict(zip(this_units.index, [session.spike_times[ii] for ii in this_units.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that PCA operates on a (ùëá,ùëõ) matrix X, where T is the number of timepoints and n is the number of dimensions.\n",
    "\n",
    "For a spiketrain, we typically consider each \"dimension\" to be the a single unit, and each sample to be the number of times that unit fires (i.e. its firing rate) within a specified time bin. \n",
    "\n",
    "The timescale you choose here matters; it will govern the timescale of the dynamics you are able to see. To start, lets choose a timescale of 50 ms bins (.05 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, lets use 50 ms bins.\n",
    "dt = .05\n",
    "\n",
    "# Take the time when the animal is activly behaving\n",
    "active_stims = session.stimulus_presentations[session.stimulus_presentations.stimulus_block==0 ]\n",
    "\n",
    "# Define time bins that span this period with intrerval dt\n",
    "session_start_time = np.min(active_stims.start_time)\n",
    "session_end_time = np.max(active_stims.start_time) \n",
    "tme = np.arange(session_start_time,session_end_time,dt)\n",
    "\n",
    "# Bin data from each unit to build X\n",
    "X = np.zeros((len(this_units),len(tme)-1))\n",
    "for ii,un in enumerate(this_units.index):\n",
    "    my_spiketimes = this_spiketimes[un]\n",
    "    X[ii,:] = np.histogram(my_spiketimes,tme)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap of the activitiy of all the cells\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "rng = int(30/dt)\n",
    "cax = ax.pcolormesh(X[:,:rng],\n",
    "                    vmin = 0, vmax = np.percentile(X[:,:rng], 99),\n",
    "                    cmap = 'magma')\n",
    "ax.set_xticks(np.arange(0,rng,50))\n",
    "ax.set_xticklabels(np.arange(0,dt*rng,dt*50))\n",
    "\n",
    "# label axes \n",
    "ax.set_ylabel('cell #')\n",
    "ax.set_xlabel('time (sec)')\n",
    "\n",
    "# creating a color bar\n",
    "cb = plt.colorbar(cax, pad=0.015, label='Spikes/bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running PCA, the data in each dimension should have a mean zero. Otherwise, large values offset from zero can and will dominate our eigenvalue decomposition, making any results tricky to interpret! \n",
    "\n",
    "In practice, sklearn automatically does this (you can turn it off for very special cases), but we have externalized it here to help with understanding how PCA works.\n",
    "\n",
    "The data should be in the form of (N,T) where N is dimension (in this case the number of neurons) and T is the number of samples (i.e. the number of time bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean of each dimension\n",
    "X = (X.T-np.mean(X.T,axis = 0))\n",
    "print(X.shape) # (N,D)\n",
    "np.mean(X,axis =0)#<--realy small numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersize (1): Now, fit a pca on X for our behavior data. For now, don't limit the number of principle components returned.\n",
    "\n",
    "HINT: The code to do this is above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to provideing the eigenvalue for your data as `explained_variance_`, the sklearn PCA object will compute the fraction of variance explained by each eigenvalue as `explained_variance_ratio_`. This is just the each eigenvalue divided by the sum of all eigenvalues to make it more interpretable.\n",
    "\n",
    "#### Exersize (2): Plot the explained variance ratio for each princple component. It is easier to see if you plot them as dots. (marker = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersize (3): Finally, transform the data X into princple components space.\n",
    "\n",
    "So that your code will work with the rest of this tutorial, save the transformed results as \"proj.\" \n",
    "\n",
    "Try plotting the first 2 dimensions of \"proj.\" What does the structure of the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "## Next, Lets take a closer look at what the PCs tell us about our population of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we binned time above, we also need to bin some of our behavioral features\n",
    "# For continuous data, we can interpolate in one dimension\n",
    "from scipy.interpolate import interp1d\n",
    "running_speed= interp1d(session.running_speed.timestamps,session.running_speed.speed)(tme[:-1])\n",
    "# For discrete data, we can use the same np.histogram command we used above\n",
    "licks  = np.histogram(session.licks,tme)[0]>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some behavior relative to the \n",
    "licks  = np.histogram(session.licks,tme)[0]>0\n",
    "plt.figure()\n",
    "plt.scatter(proj[licks==0,0],proj[licks==0,1],label = 'no lick')\n",
    "plt.scatter(proj[licks==1,0],proj[licks==1,1],label = 'lick')\n",
    "plt.legend()\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(proj[licks==0,0],density=True,label = 'no licks',alpha = .7)\n",
    "plt.hist(proj[licks==1,0],density=True,label = 'licks',alpha = .7)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('Fraction of bins')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running speed\n",
    "running_speed= interp1d(session.running_speed.timestamps,session.running_speed.speed)(tme[:-1])\n",
    "plt.figure()\n",
    "plt.scatter(proj[:,0],proj[:,1],c = running_speed,marker= '.')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2') \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Running speed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Lets take a moment to think about what we have done here. PCA looks for the dimension that describes the most variance in our data - in this case the activity of some neurons. We never explicity gave our algorithm any information about licking or running!\n",
    "    </p>\n",
    "    <p>\n",
    "This is a demonstration about why PCA and algorithms like it are refered to as unsupervised learning. They do not consider outside labeling or annotation, but instead rely solely on the inherent structure in the data. This can (sometimes) facilitate understand of what is going on in a dataset.\n",
    "    </p>\n",
    "    <p>\n",
    "\n",
    "There are many other unsupervised learning methods.  They are helpful when trying to either reduce the complexity of the dataset in order to facilitate another computation (such as regression or classification) or for exploratory data analysis in which one is trying to discover structure that may be obscured by noise or not immediately clear due to high dimensionality.\n",
    "    </p>\n",
    "    <p>\n",
    "One of the <b> very </b> nice things about the sklearn interface is that it makes it easy to try different unsupervised models. Each model is built around a standard interface with a standard set of commands (e.g. `.fit`, `.transform`, etc.). A list of other unsupervised models implemented in sklean is avalible here:      https://scikit-learn.org/stable/supervised_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets look at the make up of our principal components. Do they typically describe the activity of one or many cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First principal compnent. \n",
    "plt.plot(pca.components_[0],label = 'PC1')\n",
    "plt.xlabel(\"Cell #\", fontsize=14)\n",
    "plt.ylabel(\"Projection strength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First principal compnent. \n",
    "plt.plot(pca.components_[0],label = 'PC1')\n",
    "#Subsequent components. \n",
    "plt.plot(pca.components_[1],label = 'PC2')\n",
    "plt.plot(pca.components_[2],label = 'PC3')\n",
    "plt.plot(pca.components_[3],label = 'PC4')\n",
    "\n",
    "plt.xlabel(\"Cell #\", fontsize=14)\n",
    "plt.ylabel(\"Projection strength\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
