{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Dimensionality Reduction SWDB 2022 </h1> \n",
    "<h3 align=\"center\">Monday, August 29, 2022</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages we will use.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In this tutorial, we look at dimensionality reduction. Dimensionality reduction is the process of taking high dimesnional data (such as a population of recorded neurons) and trasforming it it into a space that is lower dimesional and (hopefully) easier to understand. We will go over one of the most common and important forms of dimensionality reduction; <b>Principal Components Analysis</b> (PCA).\n",
    "</p>\n",
    "    \n",
    "<p> PCA works by identifying the dimensions of a dataset that contain the most variance (more on this to come!). Because this dimensionality reduction is based on the inherent structure of the data, it is one of the most common forms of <b>unsupervised learning</b>.  In contrast to supervised learning, unsupervised learning operates on a set of data points without labels or ids.  Instead of trying to find a transformation between the data and labels attached to the data, we seek to find a transformation that discovers structure in the data.\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "Lets assume the data $\\vec{x}_i$ exists in $N$ dimensions.  Given an integer $n < N$, PCA attemps to find a linear subspace of dimension $n$ that minimizes the variance of the data outside of that subspace.  Put another way, PCA defines a subspace of dimension $n < N$ such that, when the data is approximated by projecting onto that subspace, the reconstruction error is minimized. In short, PCA finds the dimensions along which the data is most spread out. These dimensions are not necessarily those present within the data, but more often are linear combinations of them.\n",
    "</p>\n",
    "    \n",
    "<p>\n",
    "Let us denote the subspace with the orthogonal matrix ${\\bf W}$, of dimensions $(n, N)$.  The data can be represented with the matrix ${\\bf X}$, of dimensions $(T, N)$, where $T$ is the number of samples.  Let the coordinates of the data in the subspace be labelled ${\\bf Z}$, which is of dimensions $(T, n)$.  The cost function for PCA is then\n",
    "</p>\n",
    "<p>\n",
    "$E = \\frac{1}{2}  \\left | {\\bf X} - {\\bf Z} \\cdot {\\bf W} \\right |^2$\n",
    "</p>\n",
    "<p>\n",
    "    Note that we have to optimize over <i>both</i> ${\\bf Z}$ and ${\\bf W}$, subject to the constraint that ${\\bf W}$ is orthogonal.\n",
    "</p>\n",
    "<p>\n",
    "We can equivalently define PCA by specifying the principal components as the eigenvectors of the covariance matrix with the $n$ largest eigenvalues.  Intuitively one can see that this choice will produce the smallest amount of variability away from the subspace, as the eigenvectors themselves indicate the direction of maximum variability, and thus will solve the problem as we originally defined it.  This allows us to compute the PCAs very simply with diagonalization or singular value decomposition.\n",
    "</p>\n",
    "<p>\n",
    "    There are many other unsupervised learning methods.  They are helpful when trying to either reduce the complexity of the dataset in order to facilitate another computation (such as regression or classification) or for exploratory data analysis in which one is trying to discover structure that may be obscured by noise or not immediately clear due to high dimensionality.\n",
    " </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p>\n",
    "The explaination above can be a little mistifying if you haven't had enough coffee and/or if it has been a minute since your last linear algebra class. We will borrow a more intuitve example posted by #AllisonHorst on twitter that some people find helpful. Consider this whale shark:\n",
    "</p>\n",
    "\n",
    "<img src=\"./resources/PCAWhale1.png\">  \n",
    "    \n",
    "<p>\n",
    "Now, imagine that the whale needs to eat this school of krill:\n",
    "</p>\n",
    "\n",
    "<img src=\"./resources/PCAWhale2.png\">  \n",
    "    \n",
    "<p>\n",
    "Principal components analysis provides a transform from krill-space into whale-shark-space. In other words, it tells us how much the shark should rotate and stretch its mouth so that it can encompas all the krill.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "    As in the last tutorial, we use <code>scikit-learn</code> to perform a Principal Components Analysis, first on some toy data, and then on some data from the Behavior recordings. The interface will be the same for other algorithms in <code>scikit-learn</code>.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some toy data. You may notice a striking similarity to the krill above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some random 2 dimensional data\n",
    "np.random.seed(1)\n",
    "X = np.dot(np.random.random(size=(2, 2)), np.random.normal(size=(2, 300))).T\n",
    "\n",
    "#plot random data\n",
    "plt.plot(X[:, 0], X[:, 1], '.');\n",
    "plt.ylim(-3,3);\n",
    "plt.xlim(-3,3);\n",
    "\n",
    "# Note that this dataset has a mean of 0 in both dimensions. This will be important later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X now has 300 data points, each with 2 dimensions\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Covariance Matrix. \n",
    "# Because X has 2 dimensions, it will be a square 2x2 matrix\n",
    "cov = (1.0/X.shape[0])*np.dot(X.T, X)\n",
    "# Alternativly: A = np.cov(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, do the eigenvalue decomposition of this matrix\n",
    "evalues, evectors = np.linalg.eig(cov)\n",
    "print('evalues: ' + str(evalues))\n",
    "print('evectors: ' + str(evectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Lets look at how these eigenvectors fall relative to our data\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.plot(X[:, 0], X[:, 1], '.')\n",
    "for value, vector in zip(evalues, evectors.T):\n",
    "    ax.plot([0, 3.*np.sqrt(value)*vector[0]], [0, 3.*np.sqrt(value)*vector[1]], '-k', lw=3);\n",
    "\n",
    "ax.set_ylim(-3,3);\n",
    "ax.set_xlim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "In this case, our PCA is an orthogonal coordinate system transformation that prioritizes maximum variance. However, one can already see that the majority of the variance in our data is explained by a single principal component!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the values onto the each of the principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto first eigenvector\n",
    "X_proj_1 = np.dot(X, evectors.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have one projection value for each data point\n",
    "X_proj_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize!\n",
    "plt.plot(X_proj_1, np.zeros(X_proj_1.shape), '.');\n",
    "plt.xlim(-3,3);\n",
    "plt.xlabel(\"First principal component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project data onto the second eigenvector\n",
    "X_proj_2 = np.dot(X, evectors.T[1])\n",
    "\n",
    "# Visualize!\n",
    "plt.plot(X_proj_2, np.zeros(X_proj_2.shape), '.');\n",
    "plt.xlim(-3,3);\n",
    "plt.xlabel(\"Second principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the day, all our principal components transform has done is rotate the data into a new coordinate space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_proj_1, X_proj_2, '.');\n",
    "plt.xlabel(\"First principal component\", fontsize=14)\n",
    "plt.ylabel(\"Second principal component\", fontsize=14)\n",
    "plt.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "### Now let's use sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has a number prebuilt tools for dimensionality reduction. For now, we are just importing PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, declare a PCA object that can be used for computing PCAs\n",
    "pca = PCA(n_components=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, do a dimensionality reduction on X\n",
    "pca.fit(X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PCA object automatically does the same computation we did above, though it uses some different nomenclature\n",
    "# the Eigenvalues are stored as 'explained variance_'\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and the eigenvectors are stored as \"components\"\n",
    "pca.components_\n",
    "# Compare this to the values above... are they the same? (Hint: They should be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot these data to verify that everything matches up.\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "ax.plot(X[:, 0], X[:, 1], 'o')\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    ax.plot([0, v[0]], [0, v[1]], '-k', lw=3);\n",
    "\n",
    "ax.set_ylim(-3,3);\n",
    "ax.set_xlim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at the data projected onto the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_project = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This, too, should look a lot like the plot above.\n",
    "plt.plot(X_project[:,0], X_project[:,1], 'o');\n",
    "plt.xlabel(\"First principal component\", fontsize=14);\n",
    "plt.ylabel(\"Second principal component\", fontsize=14);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first principal component explains most of the variance, we can also project our two-dimensional data into a one-dimensional space (making use of the dimensionality-reduction that can be accomplished with PCA!) Note the similarity between this projection and the full data. This will be similar to what happens to high-dimensional data that is reduced to two dimensions for the examples below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting only the first principal component\n",
    "plt.plot(X_project[:,0], np.zeros(len(X_project[:,0])), 'o');\n",
    "plt.xlabel(\"First principal component\", fontsize=14);\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "## Let's try this with some real data from the Visual Behavior dataset!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allensdk\n",
    "from allensdk.brain_observatory.\\\n",
    "    behavior.behavior_project_cache.\\\n",
    "    behavior_neuropixels_project_cache \\\n",
    "    import VisualBehaviorNeuropixelsProjectCache\n",
    "import os\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "data_dirname = 'visual-behavior-neuropixels'\n",
    "use_static = False\n",
    "if 'Darwin' in platstring or 'macOS' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2022/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on AWS\n",
    "    data_root = \"/data/\"\n",
    "    data_dirname = 'visual-behavior-neuropixels-data'\n",
    "    use_static = True\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2022/\"\n",
    "\n",
    "# get the cache location\n",
    "cache_dir = os.path.join(data_root, data_dirname)\n",
    "\n",
    "# Manual entry if you you are doing things differently...\n",
    "cache_dir = \"../Data\"\n",
    "\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to find which units have the sessions we are interested in, so we will need the units and sessions tables.\n",
    "units_table = cache.get_unit_table()\n",
    "ecephys_sessions_table = cache.get_ecephys_session_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we want to find a \"Good\" Session to look at;\n",
    "# For now, we are going to grab the one with the most MRN units.\n",
    "unit_by_session = units_table.join(ecephys_sessions_table,on = 'ecephys_session_id')\n",
    "unit_in = unit_by_session[(unit_by_session['structure_acronym']=='MRN') & (unit_by_session['experience_level']=='Familiar')]\n",
    "unit_count = unit_in.groupby([\"ecephys_session_id\"]).count()\n",
    "familiar_session_with_most_in_units = unit_count.index[np.argmax(unit_count['ecephys_probe_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually imort the data\n",
    "session = cache.get_ecephys_session(ecephys_session_id=familiar_session_with_most_in_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unit information\n",
    "session_units = session.get_units()\n",
    "# Channel information\n",
    "session_channels = session.get_channels()\n",
    "# And accosiate each unit with the channel on which it was found with the largest amplitude\n",
    "units_by_channels= session_units.join(session_channels,on = 'peak_channel_id')\n",
    "\n",
    "# Filter for units in primary visual cortex\n",
    "this_units = units_by_channels[(units_by_channels.structure_acronym == 'MRN')\\\n",
    "                               &(units_by_channels['isi_violations']<.5)\\\n",
    "                               &(units_by_channels['amplitude_cutoff']<0.1)\\\n",
    "                               &(units_by_channels['presence_ratio']>0.95)]\n",
    "# Get the spiketimes from these units as a dictionary\n",
    "this_spiketimes = dict(zip(this_units.index, [session.spike_times[ii] for ii in this_units.index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that PCA operates on a (𝑇,𝑛) matrix X, where T is the number of timepoints and n is the number of dimensions.\n",
    "\n",
    "For a spiketrain, we typically consider each \"dimension\" to be the a single unit, and each sample to be the number of times that unit fires (i.e. its firing rate) within a specified time bin. \n",
    "\n",
    "The timescale you choose here matters; it will govern the timescale of the dynamics you are able to see. To start, lets choose a timescale of 50 ms bins (.05 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, lets use 50 ms bins.\n",
    "dt = .05\n",
    "\n",
    "# Take the time when the animal is activly behaving\n",
    "active_stims = session.stimulus_presentations[session.stimulus_presentations.stimulus_block==0 ]\n",
    "\n",
    "# Define time bins that span this period with intrerval dt\n",
    "session_start_time = np.min(active_stims.start_time)\n",
    "session_end_time = np.max(active_stims.start_time) \n",
    "tme = np.arange(session_start_time,session_end_time,dt)\n",
    "\n",
    "# Bin data from each unit to build X\n",
    "X = np.zeros((len(this_units),len(tme)-1))\n",
    "for ii,un in enumerate(this_units.index):\n",
    "    my_spiketimes = this_spiketimes[un]\n",
    "    X[ii,:] = np.histogram(my_spiketimes,tme)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap of the activitiy of all the cells\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "\n",
    "rng = int(30/dt)\n",
    "cax = ax.pcolormesh(X[:,:rng],\n",
    "                    vmin = 0, vmax = np.percentile(X[:,:rng], 99),\n",
    "                    cmap = 'magma')\n",
    "ax.set_xticks(np.arange(0,rng,50))\n",
    "ax.set_xticklabels(np.arange(0,dt*rng,dt*50))\n",
    "\n",
    "# label axes \n",
    "ax.set_ylabel('cells')\n",
    "ax.set_xlabel('time (sec)')\n",
    "\n",
    "# creating a color bar\n",
    "#cb = plt.colorbar(cax, pad=0.015, label='dF/F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running PCA, the data in each dimension should have a mean zero. Otherwise, large values offset from zero can and will dominate our eigenvalue decomposition, making any results tricky to interpret! \n",
    "\n",
    "In practice, sklearn automatically does this (you can turn it off for very special cases), but we have externalized it here to help with understanding how PCA works.\n",
    "\n",
    "The data should be in the form of (N,T) where N is dimension (in this case the number of neurons) and T is the number of samples (i.e. the number of time bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the mean of each dimension\n",
    "X = (X.T-np.mean(X.T,axis = 0))\n",
    "print(X.shape) # (N,D)\n",
    "np.mean(X,axis =0)#<--realy small numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exersize (1): Now, fit a pca on X for our behavior data.\n",
    "\n",
    "HINT: The code to do this is above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exersize (2): Next, plot the explained variance ratio for each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exersize (3): Finally, transform the data X into princple components space.\n",
    "\n",
    "So that your code will work with the rest of this tutorial, save the transformed results as \"trans.\" \n",
    "\n",
    "Try plotting the first 2 dimensions of \"trans.\" What does the structure of the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "## Next, Lets take a closer look at what the PCs tell us about our population of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we binned time above, we also need to bin some of our behavioral features\n",
    "# For continuous data, we can interpolate in one dimension\n",
    "from scipy.interpolate import interp1d\n",
    "running_speed= interp1d(session.running_speed.timestamps,session.running_speed.speed)(tme[:-1])\n",
    "# For discrete data, we can use the same np.histogram command we used above\n",
    "licks  = np.histogram(session.licks,tme)[0]>0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some behavior relative to the \n",
    "licks  = np.histogram(session.licks,tme)[0]>0\n",
    "plt.figure()\n",
    "plt.scatter(trans[:,0],trans[:,1],c = licks)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "cbar = plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(trans[licks==1,0],density=True,label = 'licks')\n",
    "plt.hist(trans[licks==0,0],density=True,label = 'no licks')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('Fraction of bins')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running speed\n",
    "running_speed= interp1d(session.running_speed.timestamps,session.running_speed.speed)(tme[:-1])\n",
    "plt.figure()\n",
    "plt.scatter(trans[:,0],trans[:,1],c = running_speed,marker= '.')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2') \n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_ylabel('Running speed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets look at the make up of our principal components. Do they typically describe the activity of one or many cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First principal compnent. \n",
    "plt.plot(pca.components_[0])\n",
    "plt.xlabel(\"Cell #\", fontsize=14)\n",
    "plt.ylabel(\"Projection strength\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First principal compnent. \n",
    "plt.plot(pca.components_[1])\n",
    "plt.plot(pca.components_[2])\n",
    "plt.plot(pca.components_[3])\n",
    "\n",
    "plt.xlabel(\"Cell #\", fontsize=14)\n",
    "plt.ylabel(\"Projection strength\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
