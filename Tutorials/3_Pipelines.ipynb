{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">Pipeline Tutorial SWDB 2022 </h1> \n",
    "<h3 align=\"center\">Monday, August 29, 2022</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection with Scikit-Learn pipelines\n",
    "\n",
    "The last few tutorials have introduced tools that are built into sklearn. This one is a little different; here, we will learn how to string the tools that we have already seen into useful data pipelines.\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "\n",
    "- How to use scikit-learn \"Pipeline\" to chain together multiple steps in a decoding analysis\n",
    "- How to cross-validate your pipeline\n",
    "- How to search hyperparameters to optimize your model\n",
    "- How to apply your pipeline to new data\n",
    "\n",
    "This tutorial is going to work through building a model to decode the identity of images as durring the \"Visual Behavior\" task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import allensdk\n",
    "from allensdk.brain_observatory.\\\n",
    "    behavior.behavior_project_cache.\\\n",
    "    behavior_neuropixels_project_cache \\\n",
    "    import VisualBehaviorNeuropixelsProjectCache\n",
    "import os\n",
    "import platform\n",
    "platstring = platform.platform()\n",
    "\n",
    "data_dirname = 'visual-behavior-neuropixels'\n",
    "use_static = False\n",
    "if 'Darwin' in platstring or 'macOS' in platstring:\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2022/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on AWS\n",
    "    data_root = \"/data/\"\n",
    "    data_dirname = 'visual-behavior-neuropixels-data'\n",
    "    use_static = True\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2022/\"\n",
    "\n",
    "# get the cache location\n",
    "cache_dir = os.path.join(data_root, data_dirname)\n",
    "\n",
    "\n",
    "#cache = VisualBehaviorNeuropixelsProjectCache.from_s3_cache(cache_dir=cache_dir)\n",
    "cache = VisualBehaviorNeuropixelsProjectCache.from_local_cache(\n",
    "            cache_dir=cache_dir, use_static_cache=use_static)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets refind the \"Best\" V1 Session we used before, and get the spiketimes and behavior data from that session.\n",
    "\n",
    "There is nothing new about this code, its exactly the same as what we used in the \"Classification\" tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to find the \"familiar\" session that contains the most V1 units. \n",
    "area = 'VISp'\n",
    "# You have actually seen this code before, so we won't spend time on it...\n",
    "units_table = cache.get_unit_table()\n",
    "ecephys_sessions_table = cache.get_ecephys_session_table()\n",
    "\n",
    "# For now, we are going to grab the one with the most V! units.\n",
    "unit_by_session = units_table.join(ecephys_sessions_table,on = 'ecephys_session_id')\n",
    "unit_in = unit_by_session[(unit_by_session['structure_acronym']==area) &\\\n",
    "                          (unit_by_session['experience_level']=='Familiar') &\\\n",
    "                          (unit_by_session['isi_violations']<.5)&\\\n",
    "                          (unit_by_session['amplitude_cutoff']<0.1)&\\\n",
    "                          (unit_by_session['presence_ratio']>0.95)]\n",
    "unit_count = unit_in.groupby([\"ecephys_session_id\"]).count()\n",
    "familiar_session_with_most_in_units = unit_count.index[np.argmax(unit_count['ecephys_probe_id'])]\n",
    "# Actually imort the data\n",
    "session = cache.get_ecephys_session(ecephys_session_id=familiar_session_with_most_in_units)\n",
    "\n",
    "# Get unit information\n",
    "session_units = session.get_units()\n",
    "# Channel information\n",
    "session_channels = session.get_channels()\n",
    "# And accosiate each unit with the channel on which it was found with the largest amplitude\n",
    "units_by_channels= session_units.join(session_channels,on = 'peak_channel_id')\n",
    "\n",
    "# Filter for units in primary visual cortex\n",
    "this_units = units_by_channels[(units_by_channels.structure_acronym == area)\\\n",
    "                               &(units_by_channels['isi_violations']<.5)\\\n",
    "                               &(units_by_channels['amplitude_cutoff']<0.1)\\\n",
    "                               &(units_by_channels['presence_ratio']>0.95)]\n",
    "# Get the spiketimes from these units as a dictionary\n",
    "this_spiketimes = dict(zip(this_units.index, [session.spike_times[ii] for ii in this_units.index]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, decode the identity of individual image presentations from the population activity, we need the following\n",
    "\n",
    "- `y` - vector of integers, where each integer corresponds to a particular image\n",
    "- `X` - array of floats, where each column is the activity of a single neuron in response to a given natural scene presentation. Each row of this array is often called a \"population vector\"\n",
    "\n",
    "The process of getting to `X` from raw data (in this case, the response of a neuron) is sometimes known as \"feature engineering\". This involves multiple decisions that will be specific to the dataset at hand, but can include the time window to include, how to smooth data, whether to do dimensionality reduction on data, etc.\n",
    "\n",
    "Here, \"y\" is the same trial-by-trial image identity we used in the last tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_stims = session.stimulus_presentations[session.stimulus_presentations.stimulus_block==0 ]\n",
    "# Get an integer value for each image name\n",
    "[unq,y]= np.unique(active_stims.image_name,return_inverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we built 'X' by finding the number of spikes in the first 250 ms of the trial. For this tutorial, we will do something slightly more complicated. We are going to look at time bins after each stimulus presentation, so we will count the number of spikes 0-50ms after each presentation, 50-100ms after each presentation, etc. This is very similar to constructing a PSTH, but we are going to keep each neurons response on each trial seperate so that we can try to decode trial identify.\n",
    "\n",
    "First, we need a time vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look we want to look at time 750 ms after the start of the trial.\n",
    "dt = .05 # Time is in seconds\n",
    "tme = np.arange(0,.75+dt,dt)\n",
    "tme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now put bin the data on a trial by trial basis. `Xbins` will now have dimensions (Trials,Neurons,TimeBins). It takes a moment to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and empty variable X\n",
    "Xbins = np.zeros((len(active_stims),len(this_spiketimes),len(tme)-1))\n",
    "# This Loop is a little slow...be patient\n",
    "# Loop Through both trials and units, counting the number of spikes\n",
    "for jj,key in enumerate(this_spiketimes):\n",
    "    # Loop through the trials\n",
    "    for ii, trial in active_stims.iterrows():\n",
    "        startInd = np.searchsorted(this_spiketimes[key], trial.start_time)\n",
    "        endInd = np.searchsorted(this_spiketimes[key], trial.start_time+.75+dt)\n",
    "        # Count the number of spikes per trial. \n",
    "        Xbins[ii,jj,:] = np.histogram(this_spiketimes[key][startInd:endInd]-trial.start_time,tme)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a pipeline\n",
    "\n",
    "Now that we have `X` and `y`, we are ready to start developing our decoding pipeline.\n",
    "\n",
    "This is going to have a few major steps. We will:\n",
    "- Choose a some data to use to optimize our parameters\n",
    "- Split these data into a 'train' and test set\n",
    "- Preprocess these data (Zero mean)\n",
    "- Do dimensionality reduction\n",
    "- Fit a classifier\n",
    "These are all things you have seen in past tutorials; if any of this is weird, please refer back to those!\n",
    "\n",
    "Because there is a lot there, you will see that it is usefull to streamline these steps into a single pipeline!\n",
    "\n",
    "We know from the classification tutorial that the first 250 ms after stimulus onset is easily decodable in V1. To establish our pipleline, take just this portion of `Xbins`. Don't worry, we have a change to play around with the rest of these data in the excersizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sum(Xbins[:,:,tme[:-1]<=.250],axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2, \n",
    "    stratify=y, # this makes sure that our training and testing sets both have all classes in y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale the data to zero mean and unit variance\n",
    "\n",
    "Before we did this manually, but sklearn provides the `StandardScalar` that does this automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "rescaler = StandardScaler()\n",
    "X_train_rescaled = rescaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "X_train_reduced = pca.fit_transform(X_train_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "scores = cross_val_score(classifier, X_train_reduced, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble these steps into a Pipeline\n",
    "\n",
    "We now have 3 steps in our processing pipeline... rescaling, reducing, and classifying. Keeping track of these steps and their intermediate variables can get confusing.\n",
    "\n",
    "Luckily, since each of these steps use objects that conform to the scikit-learn fit/transform/predict standard, we can use the scikit learn pipeline module to assemble them into a pipeline, which itself has fit & predict methods, so it can drop into functions like `cross_val_score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline((\n",
    "    ('rescaler', StandardScaler()),\n",
    "    ('reducer', PCA(n_components=20)),\n",
    "    ('classifier', LinearDiscriminantAnalysis()),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the pipeline\n",
    "\n",
    "Now that we've defined a pipeline, we can more readily adjust parts of the pipeline to improve it... adjust the parameters for each step, remove steps, replace one classifier with another. These pieces of an ML pipeline are aften referred to as \"hyperparameters\".\n",
    "\n",
    "If we want to do this exploration strategically, we can search over this space to select the best hyperparameters. One way to do this is with scikit-learn's `GridSearchCV`. This lets us do the following....\n",
    "\n",
    "- specify a set of parameters we want to search over\n",
    "- for each combination of parameters, do an n-fold cross validation\n",
    "- rank the parameter combinations based on their average cross validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "N_FEATURES_OPTIONS = [20, 40, 80,]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'rescaler': [None, StandardScaler()],\n",
    "        'reducer__n_components': N_FEATURES_OPTIONS,\n",
    "        'classifier': [\n",
    "            LinearDiscriminantAnalysis(),\n",
    "            KNeighborsClassifier(n_neighbors=50),\n",
    "            DecisionTreeClassifier(),\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` stores the scores from each cross validation run, so we can go at our parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cv_results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['param_rescaler'] = ~pd.isnull(cv_results['param_rescaler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results['param_classifier'] = cv_results['param_classifier'].map(lambda x: str(x).split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.factorplot('param_reducer__n_components','mean_test_score',data=cv_results,\n",
    "               hue='param_classifier',\n",
    "               col='param_rescaler',\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `GridSearchCV` and a fit `Pipeline` objects has the their own internal scikit-learn predict & score methods. \n",
    "\n",
    "In the case `GridSearchCV`, this will use the \"best estimator\" that was identified during the fitting step.\n",
    "\n",
    "So now we can use the score method to evaluate the model on the held out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativly, we can extract the pipeline and use it explicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = grid.best_estimator_\n",
    "pipeline.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pipeline object\n",
    "One of the distinct advangages of the pipeline object is that you can readily fit a given pipeline to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersize (1): Earlier, we built an Xbins array. Loop through each in this array and cross validate the pipeline we just built using this new data. Plot the scores relative to the time from stimulus onset. \n",
    "\n",
    "Here is some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 5 fold cross validation\n",
    "scores = np.zeros((5,Xbins.shape[2]))\n",
    "for tt in range(Xbins.shape[2]):\n",
    "    this_X = Xbins[:,tt]  \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "# Do your plotting here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pipeline can also save you time by streamline preprocessing so you can compare the performance of a decoder trained under one condition to a dataset collected under another condition .This is sometimes a good way to look for changes in population activity between conditions. \n",
    "\n",
    "Lets say, for example, you want to train a decoder on image identify durring the 250 ms after stimulus onset, and test it with data between 500 and 750 ms after stimulus onset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum accross the requisite parts of X to build the matricies described above\n",
    "X250 = np.sum(Xbins[:,:,tme[:-1]<=250],axis = 2) \n",
    "X750 = np.sum(Xbins[:,:,tme[:-1]>500],axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exersize (2): fit out pipeline using 'X250', then compare its performance in predicting image identify from 'X750'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
