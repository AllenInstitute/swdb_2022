{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102cd20e",
   "metadata": {},
   "source": [
    "###### <img src=\"../resources/cropped-SummerWorkshop_Header.png\">  \n",
    "\n",
    "<h1 align=\"center\">PyTorch Tutorial SWDB 2022 </h1> \n",
    "<h3 align=\"center\">Tuesday, August 30, 2022</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb778612",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "Many of the problems and methods we have seen rely on optimizations that use some form of gradient descent.  For a reminder, gradient descent methods update the parameters of some model according to the negative of the gradient of a cost function with respect to those parameters, in order to minimize that cost function.  \n",
    "</p>\n",
    " </div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af11b7",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "If you are using the Jupyter Hub, PyTorch has already been installed for you.  If you are running python on your laptop, you will need to install PyTorch.\n",
    "</p>    \n",
    "\n",
    "<p>\n",
    "To install pytorch, run the following cell for macOS\n",
    "</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74953bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pytorch torchvision torchaudio -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819570e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "or the following cell for Linux or Windows\n",
    "</p>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d9bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feade21",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "\n",
    "For simple functions and parameter sets, this computation is not that hard to compute.  Let's look at an example.  Here we will explicitly build a classifier by hand that we discussed in the Classification tutorial. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef370f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classes(X,y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    color = 'rbgmyk'\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    for cl in range(num_classes):\n",
    "        ax.scatter(X[y==cl,0],X[y==cl,1],c=color[cl],edgecolor='none')\n",
    "    ax.set_xlim(X[:,0].min(),X[:,0].max())\n",
    "    ax.set_ylim(X[:,1].min(),X[:,1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_features=2,n_redundant=0,random_state=1,n_samples=1000)\n",
    "        \n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classes(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b77ca1",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p>\n",
    "The two classes above are labeled `0` and `1`.  Logistic Regression is a form of binary classification that computes the probability of membership in class `1` according to the following form:\n",
    "\n",
    "$ P_1(x) = \\frac{1}{1 + e^{-\\vec{w}\\cdot \\vec{x} - b}}$\n",
    "\n",
    "with $P_2 = 1 - P_1$.  Fitting this model requires selecting the vector $\\vec{w}$ and the constant $b$ to minimize the *negative log likelihood*:\n",
    "\n",
    "$ {\\rm NLL} = -\\sum_i n_i \\log P_1(x_i) + (1-n_i) \\log P_2(x_i) $\n",
    "\n",
    "This is the same as maximizing the probability that the model produced the observed data.  \n",
    "\n",
    "In order to perform gradient descent, we need to update the parameters $\\vec{w}$ and $b$ according to the gradient of ${\\rm NLL}$ with respect to those parameters.  This example algorithm is simple both conceptually and mathematically, and therefore not difficult to implement in python using `numpy` arrays.  For this model that is given in the cell below:\n",
    "    </p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(w,b):\n",
    "    return 1.0/(1.0 - np.exp(-np.dot(X,w) - b))\n",
    "\n",
    "def grad_w(w,b):\n",
    "    return -np.mean(X.T*(y - prob(w,b)), axis=1)\n",
    "\n",
    "def grad_b(w,b):\n",
    "    return -np.mean((y - prob(w,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756da14",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "We use these gradients to perform gradient descent. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef583ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(2)\n",
    "b = np.random.random()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1000):\n",
    "    w_last = w\n",
    "    b_last = b\n",
    "    \n",
    "    w = w_last - learning_rate*grad_w(w_last, b_last)\n",
    "    b = b_last - learning_rate*grad_b(w_last, b_last)\n",
    "    \n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prob(w,b)>0\n",
    "\n",
    "plot_classes(X, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d7394",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "This gives a pretty decent separation of the two classes, aside from the overlapping region.  What about a problem where the classes are clearly not linearly separable?\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb91666",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(noise=0.1, factor=0.5, random_state=1,n_samples=2000)\n",
    "\n",
    "plot_classes(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e333aeb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Let's do the computation again with the same gradients and code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21732ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(w,b):\n",
    "    return 1.0/(1.0 - np.exp(-np.dot(X,w) - b))\n",
    "\n",
    "def grad_w(w,b):\n",
    "    return -np.mean(X.T*(y - prob(w,b)), axis=1)\n",
    "\n",
    "def grad_b(w,b):\n",
    "    return -np.mean((y - prob(w,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(2)\n",
    "b = np.random.random()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1000):\n",
    "    w_last = w\n",
    "    b_last = b\n",
    "    \n",
    "    w = w_last - learning_rate*grad_w(w_last, b_last)\n",
    "    b = b_last - learning_rate*grad_b(w_last, b_last)\n",
    "    \n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prob(w,b)>0\n",
    "\n",
    "plot_classes(X, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09139470",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "Since this problem is not linearly separable, we have no chance of correctly fitting the data.  We can create a more complex model, but we will need to compute the gradients of the cost function with respect to more parameters.  This can quickly become unwieldy.  \n",
    "\n",
    "Let's consider adding a single hidden layer to our model.  Now we have many parameters, and we have to consider the chain rule when deriving these gradients.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c947c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.shape = (2,num_hidden)\n",
    "# b.shape = (num_hidden,)\n",
    "# w.shape = (num_hidden,)\n",
    "\n",
    "def hidden(m, b):\n",
    "    temp = X.dot(m) + b\n",
    "    temp[temp<0] = 0\n",
    "    return temp    # shape is [samples, number_hidden]\n",
    "\n",
    "def output(w, m, b):\n",
    "    return hidden(m,b).dot(w)   # shape is [samples]\n",
    "\n",
    "def prob(w,m,b):\n",
    "    return 1.0/(1.0 + np.exp(-output(w,m,b)))\n",
    "\n",
    "def grad_w(w,m,b):\n",
    "    error = y - prob(w,m,b)\n",
    "    return -hidden(m,b).T.dot(error)/len(y)\n",
    "\n",
    "def grad_m(w,m,b):\n",
    "    hidden_temp = X.dot(m) + b\n",
    "    relu_prime = np.ones_like(hidden_temp)\n",
    "    relu_prime[hidden_temp<0] = 0\n",
    "    error = y - prob(w,m,b) \n",
    "    A = relu_prime.T*error\n",
    "    return -X.T.dot(A.T)*w/len(y)\n",
    "\n",
    "def grad_b(w,m,b):\n",
    "    hidden_temp = X.dot(m) + b\n",
    "    relu_prime = np.ones_like(hidden_temp)\n",
    "    relu_prime[hidden_temp<0] = 0\n",
    "    error = y - prob(w,m,b) \n",
    "    A = relu_prime.T*error\n",
    "    return -np.mean(A.T*w, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 100\n",
    "w0 = np.random.random(num_hidden)\n",
    "m0 = np.random.random((2,num_hidden))\n",
    "b0 = np.random.random(num_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w0\n",
    "b = b0\n",
    "m = m0\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for t in range(1000):\n",
    "    w_last = w\n",
    "    b_last = b\n",
    "    m_last = m\n",
    "    \n",
    "    w = w_last - learning_rate*grad_w(w_last, m_last, b_last)\n",
    "    b = b_last - learning_rate*grad_b(w_last, m_last, b_last)\n",
    "    m = m_last - learning_rate*grad_m(w_last, m_last, b_last)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bf369",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prob(w,m,b)>0.5\n",
    "\n",
    "plot_classes(X, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca78fc",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "That worked!  But that was a LOT of work for a very simple model.  What if we wanted more than one hidden layer?  What if we wanted more complex types of computations such as convolutions?  In that case we would need to compute a complex set of derivatives and implement them individually for EACH model we wanted to consider.  \n",
    "\n",
    "Similar to how `numpy` solves fast computation with `array`s and `pandas` provides access to relational data algorithms via the `DataFrame` object, PyTorch solves the problem of computing gradients by providing the `Tensor` object.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421298de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4eda2f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "In many respects, `Tensor`s look and behave similarly to `numpy` `array`s, with many of the same methods and behaviors.  (Warning: in some cases there is a change in method or argument name for what is essentially the same behavior in `numpy`.)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf43a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([3,5,6,7])\n",
    "print(\"x = \", x)\n",
    "\n",
    "y = torch.Tensor([8,6,3,2])\n",
    "print(\"y = \", y)\n",
    "\n",
    "print(\"x + y = \", x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae8782",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "You can access values with bracket notation and you can use slicing in the same way.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9381da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x[3] = \", x[3])\n",
    "print(\"x[1:3] = \", x[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7922b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "There are similar construction methods as `array`s.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15a3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"torch.arange(10): \", torch.arange(10))\n",
    "print()\n",
    "print(\"torch.ones(2,3):  \\n\", torch.ones(2,3))\n",
    "print()\n",
    "print(\"torch.zeros(2,5,6):  \\n\", torch.zeros(2,5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85002e84",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "`Tensors` have shapes similar to `array`s, with the same conventions for axes.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(2,5,6)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2221c969",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "There are also similar operations.   (Note that the optional argument below is called `dim` and not `axes`.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4422f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(z, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc871b7a",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "We can create `Tensor`s by initializing with `array`s.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c94bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy = np.random.random((3,3))\n",
    "\n",
    "x = torch.Tensor(x_numpy)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41683060",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "We can also use the `from_numpy` method to convert from an `array` to a `Tensor`.  `Tensor`s created this way share the same data as the `array`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9a5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(x_numpy)\n",
    "\n",
    "print(\"Tensor:  \\n\", x)\n",
    "x[:] = 0\n",
    "print(\"Tensor:  \\n\", x)\n",
    "print(\"Array:  \\n\", x_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db20ec5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Like `array`s, `Tensor`s have data types.  By default they are of type `torch.float64`.  If you wish to use GPUs, you may wish to convert these to `torch.float32`.  You can convert them with the `to` method or initialize them with the desired data type.  Note that `to` does not act in place.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.dtype)\n",
    "\n",
    "x = x.to(torch.float32)\n",
    "\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones((3,3), dtype=torch.float32)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d0018",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Creating an `array` from a `Tensor` can be done with the `.numpy` method.  The resulting `array` will share the same memory space as the `Tensor`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "xn = x.numpy()\n",
    "print(x)\n",
    "print(type(x))\n",
    "\n",
    "x[:] = 0\n",
    "print(xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06770728",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "There are also convenience functions defined in PyTorch similar to those defined in `numpy`.  Many are particularly useful for machine learning applications.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c00047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = torch.linspace(-3, 3, 100)\n",
    "ys0 = torch.relu(xs)\n",
    "ys1 = torch.sigmoid(xs)\n",
    "ys2 = torch.tanh(xs)\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(8,2))\n",
    "ax[0].plot(xs, ys0)\n",
    "ax[1].plot(xs, ys1)\n",
    "ax[2].plot(xs, ys2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f0f3c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "<h2> Automatic Differentiation </h2>\n",
    "\n",
    "You might be wondering why we defined a whole new object that behaves exactly like one we already had.  \n",
    "\n",
    "`Tensor`s are designed for *automatic differentiation*.  If we want to define a cost function for a multi-layer network, or any composition of complex functions, we will have a *computation graph* that defines the relationships among the different functions.  The chain rule of calculus then provides a similar set of relationships in order to compute the gradients of the parameters in that graph with respect to some defined cost function.  As long as the appropriate derivative is defined for each operation, automatic differentiation is the process of combining these derivatives according to the chain rule and the computation graph to get the required gradients for each parameter.  Even better, this can happen without any direct user involvement.  The entire computation happens under the hood.\n",
    "\n",
    "How do `Tensor`s allow this?  They have a `data` property that contains the numerical data and they have a `grad` property that contains the gradient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c901587",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3)\n",
    "\n",
    "print(x.data)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22bf09",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "`grad` is a parameter that the tensor uses to store the gradient of some cost function with respect to the variable the `Tensor` represents. \n",
    "\n",
    "Why do we get `None` above?  `Tensor`s have a property `requires_grad` that determines whether that `Tensor` should be considered a variable that we wish to compute the gradient with respect to, or should be considered a constant.  By default, `requires_grad` is False, but we can define it when we create the `Tensor`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b77a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.requires_grad)\n",
    "x = torch.ones(3, requires_grad=True)\n",
    "print(x.requires_grad)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61df2fc",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "But we still get `None`.  Why?  Because we haven't defined anything that can act as a cost function!  Until a gradient is computed, `.grad` will return `None`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(3, requires_grad=True)\n",
    "\n",
    "cost = torch.sum(torch.sigmoid(x**2 + 0.5*y**2))\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70674e7",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Notice the `grad_fn` in the result above.  `grad_fn` defines the function that PyTorch will use to compute the gradient.  Since there are multiple operations that define `cost` above, several gradient computations will be nested.  We can access the computations at the next stage of the computation graph with `.next_functions`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da96e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cost.grad_fn:  \", cost.grad_fn)\n",
    "next_f = cost.grad_fn.next_functions\n",
    "print(\"next functions: \", next_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb10e8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "We can traverse the computation graph by repeatedly calling `.next_functions`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366618bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_list = [(cost.grad_fn,0)]\n",
    "\n",
    "while len(op_list)>0:\n",
    "    op, level = op_list.pop()\n",
    "    if op is not None:\n",
    "        op_list += [(next_op, level+1) for next_op, _ in op.next_functions]\n",
    "    print(\"   \"*level, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1846d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c43c4ff",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "The gradient for a cost function can be computed by calling `.backward`.  This will populate the `grad` property of any `Tensor`s that have `requires_grad==True`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e7358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844b2fb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "After calling `backward`, the gradients will accumulate in the `grad` property.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9462c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1f1db",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Calling `backward` twice on the same cost function will raise a `RuntimeError`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022ac0f",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "However, we can create a new computation graph be redefining the cost function. Calling backward on this will add the gradient values to those already accumulated in `grad`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost2 = torch.sum(x**2 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad79efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost2.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f189bf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "The accumulated gradients are the sums of the gradients for the two cost functions. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8252d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119d01b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "`Tensor`s with `requires_grad==True` cannot be converted to `numpy` arrays, nor can they be modified in place.  \n",
    "\n",
    "The `.detach` method returns a `Tensor` with `requires_grad==False`.  \n",
    "\n",
    "Warning: detached `Tensor`s share the same memory space and can be modified in place.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091be033",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.detach().numpy()\n",
    "print(type(x_np))\n",
    "\n",
    "print(\"tensor:  \", x)\n",
    "print(\"array:  \", x_np)\n",
    "\n",
    "x_np[:]=0\n",
    "\n",
    "print(\"tensor:  \", x)\n",
    "print(\"array:  \", x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61323b11",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "The `clone` method will give a `Tensor` with a copy of the memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd006dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clone = x.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de540aa",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Let's minimize the cost function from above.  Note that in each round through the `for` loop we are setting the gradients of each `Tensor` to zero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = torch.ones(3, requires_grad=True)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    cost = torch.sum(torch.sigmoid(x**2 + 0.5*y**2))\n",
    "    cost.backward()\n",
    "    \n",
    "    x.data = x.data - learning_rate*x.grad\n",
    "    y.data = y.data - learning_rate*y.grad\n",
    "    \n",
    "    x.grad.zero_()\n",
    "    y.grad.zero_()\n",
    "    \n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd50d5",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "<h2> GPU support </h2>\n",
    "\n",
    "One nice advantage of PyTorch is that it is very easy to use GPU computation.  The `to` method will move a `Tensor` to another device (a cpu or gpu).  In most cases, gpu support will be provided by the CUDA library from Nvidia.  You can check whether CUDA support is available for your environment by calling `torch.cuda.is_available`.\n",
    "\n",
    "All `Tensor`s in a computation must be on the same device or it will raise an error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a403a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(\"number of gpus:  \", num_gpus)\n",
    "\n",
    "for n in range(num_gpus):\n",
    "    print(torch.cuda.get_device_name(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66949da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")  # or \"cuda\"\n",
    "\n",
    "print(device)\n",
    "x_gpu = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30ec6b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "If you have a recent Mac with Apple Silicon, there is also gpu support available, *if* you are using versions of Python and PyTorch compiled for ARM cpus.  The device name is `mps` (for Metal Performance Shaders).  Use `mps` in the same way you would use `cuda`.\n",
    "\n",
    "(Note:  if you followed the setup instructions for installing Anaconda at the beginning of the course, you should have the x86 (i.e. Intel) version of Python installed.  If you want to play with gpu support on your M1/M2 laptop, ask an instructor about setting up an ARM environment.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e9f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf00c98",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "<h2> The Network module </h2>\n",
    "\n",
    "PyTorch provides a class that helps us build networks with multiple layers and complex interactions among operators.  This is in the `nn` (\"neural network\") submodule and is called `torch.nn.Module`.\n",
    "\n",
    "`torch.nn.Module` is a class with specific useful properties.  Although we've talked about objects, we have not talked about defining classes in this course.  Briefly, for the `Module` type of object, you can define your own operations for a computation graph by specifying operators in a special method called `__init__` that runs when the object is created.  The operation of the network itself is given by the `forward` function, which defines how the operations are connected together in the graph.\n",
    "\n",
    "The following example defines a network with two \"hidden\" layers with 32 units each connecting an input of `input_size` and an output of `output_size`.  `torch.nn.Linear` is an operator that defines a matrix multiplication with an additional bias term for each unit.  `torch.nn.ReLU` return an activation function that behaves like a rectified linear function.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa152f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66e5bb",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "You create the object with the following code:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b04fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3e6c3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Let's use this network to solve the classficiation problem we solved above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1252ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(noise=0.1, factor=0.5, random_state=1,n_samples=2000)\n",
    "\n",
    "plot_classes(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22081e3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Let's separate these data into train and test, then define `Tensor`s so we can train our network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49552da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:1000]\n",
    "y_train = y[:1000]\n",
    "\n",
    "X_test = X[1000:]\n",
    "y_test = y[1000:]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f270d0d",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "PyTorch includes a selection of common cost functions that we can use to train networks.  Two useful cost functions are `torch.nn.MSELoss` and `torch.nn.CrossEntropyLoss`.  You can see other cost functions provided in https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "Note that these methods return *functions*.  They are called by supplying input, target pairs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2cf087",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss =  loss_function(network(X_train_tensor),y_train_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2862c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "In addition, the `optim` submodule provides a set of different optimizers (many different ways of updating weights for each round of training).  We can use these optimizers intead of manually updating the parameters of each network.\n",
    "\n",
    "\n",
    "\n",
    "`torch.optim.SGD` is stochastic gradient descent, which is the standard simple update rule that we used above.  `lr` is the learning rate.  `.parameters` will return all of the `Tensor`s with that are trainable (i.e. have `requires_grad==True`) in a `Network`.  Calling `.step` will apply this update once gradients have been computed.\n",
    "\n",
    "Typically before calling `step` with an optimizer, we should set the gradients of all tensors to zero so we don't accumulate computations from previous steps.  We can do this by calling `.zero_grad` for the optimizer, which will the gradient of all parameters to zero.\n",
    "\n",
    "You can find other optimizers at https://pytorch.org/docs/stable/optim.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff69ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1937d727",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Let's start by looking at the initial state of this network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96921beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(network(X_test_tensor).detach().numpy(), axis=1)\n",
    "\n",
    "plot_classes(X_test_tensor, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    \n",
    "    output = network(X_train_tensor)\n",
    "    loss = loss_function(output,y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58281b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(network(X_test_tensor).detach().numpy(), axis=1)\n",
    "\n",
    "plot_classes(X_test_tensor, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7f555e",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "\n",
    "During training, you may want to periodically check how your model is doing on a test or validation set.  Sometimes there are network properties that will behave differently in training vs. evaluation situations, such as Dropout and Batch Normalization (which we aren't using here).  In this case, you can specify which mode you want the network to operate in by calling `.train` and `.test`.  In addition, using the context `torch.no_grad` will disable gradient computation, which you will usually want to do when you are evaluating.  (The python keyword `with` is new for us.  It is used to establish contexts.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898094c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "network = Net(2,2)\n",
    "network.to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 2000\n",
    "\n",
    "performance = []\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    output = network(X_train_tensor.to(device))\n",
    "    loss = loss_function(output,y_train_tensor.to(device))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = network(X_test_tensor.to(device))\n",
    "        prediction = np.argmax(network(X_test_tensor.to(device)).to(\"cpu\").detach().numpy(), axis=1)\n",
    "\n",
    "        performance.append(np.mean(prediction==y_test_tensor.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(100*np.array(performance))\n",
    "ax.set_ylabel('% correct on test')\n",
    "ax.set_xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff86669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
